
# ðŸ§  Two-Layer Neural Network from Scratch â€“ MNIST Classification

This project implements a two-layer neural network **from scratch** using NumPy to classify handwritten digits from the MNIST dataset. It is designed to give a clear, ground-up understanding of how basic neural networks work â€” without using libraries like TensorFlow or PyTorch.

---

## ðŸš€ Project Overview

- ðŸ“¦ **Dataset**: MNIST (28x28 grayscale images)
- ðŸ›  **Tools**: Python, NumPy
- ðŸ§® **Architecture**:
  - Input Layer: 784 neurons (28x28 pixels flattened)
  - Hidden Layer: 128 neurons, ReLU activation
  - Output Layer: 10 neurons, Softmax activation

---

## ðŸ§  Key Concepts Implemented

- Data loading and preprocessing
- Weight initialization
- Forward propagation
- ReLU and Softmax activations
- Cross-entropy loss
- Backpropagation
- Parameter updates with Gradient Descent
- Accuracy evaluation

---
